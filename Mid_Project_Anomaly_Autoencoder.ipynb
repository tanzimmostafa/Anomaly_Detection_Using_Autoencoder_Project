{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mid Project: Anomaly Detection with Autoencoders\n",
        "\n",
        "Dr. Leslie Kerby, CS 6699 Advanced AI Methods <br>\n",
        "Spring 2024"
      ],
      "metadata": {
        "id": "aM3uBfakkKgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this assignment is to design, implement, and evaluate an autoencoder for the purpose of anomaly detection on the [Credit Card Fraud Detection dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud), available in Kaggle. This dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
        "\n",
        "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "Note:\n",
        "- Experiment with different autoencoder architectures, including varying the number and size of layers, to find the best model for this task.\n",
        "- Discuss the importance of choosing an appropriate threshold for anomaly detection and the trade-off between false positives and false negatives."
      ],
      "metadata": {
        "id": "LTOK8ks-kWbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Data Exploration and Preprocessing:\n",
        "   - Familiarize yourself with the dataset. Understand the features and the target variable.\n",
        "   - Handle missing values if any, and normalize the data if required. Given the PCA-transformed nature of the dataset, normalization might already be taken care of, but it's good to check.\n",
        "   - Since the dataset is imbalanced, discuss how this might affect training and how you plan to address it.\n"
      ],
      "metadata": {
        "id": "cx6xNmmwmIA2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "202UOApMkK6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Model Design\n",
        "   - Design an autoencoder architecture suitable for this dataset. Given the nature of the data (numerical and possibly high-dimensional), a dense (fully connected) network might be a good starting point.\n",
        "   - Consider the size of the latent space carefully; it should be small enough to force the autoencoder to learn a compressed representation but large enough to capture the essential characteristics of normal transactions."
      ],
      "metadata": {
        "id": "PPfDEOdNkLT6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3xcrL3mkLfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Training\n",
        "   - Train your autoencoder using only the normal transactions in the training set. This is crucial as the autoencoder needs to learn to reconstruct normal transaction profiles.\n",
        "   - Validate the performance of your autoencoder on a separate set of normal transactions to tune hyperparameters and avoid overfitting."
      ],
      "metadata": {
        "id": "hRgGX8B0kLqb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6B9KvkcPkL1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Anomaly Detection\n",
        "   - Use the reconstruction error as the metric to detect anomalies. Transactions that result in a high reconstruction error are likely to be anomalies (fraudulent transactions in this case).\n",
        "   - Determine a threshold for the reconstruction error above which a transaction is considered fraudulent. This can be based on a validation set or statistical methods."
      ],
      "metadata": {
        "id": "prxLwvI_mZ44"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AV14_UV-maDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5: Evaluation\n",
        "   - Evaluate your model on a test set containing both normal and fraudulent transactions. Use metrics suitable for imbalanced classification problems, such as precision, recall, F1-score, and the area under the ROC curve (AUC).\n",
        "   - Discuss the performance of your model and any challenges you encountered."
      ],
      "metadata": {
        "id": "mKEezCRhkL_3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whsaB2bMkMKS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}